
### Viewing Resources:
`kubectl get nodes`: Lists all nodes in the cluster. <br>
`kubectl get nodes master1`
`kubectl get nodes -w`:  <br>
`kubectl get nodes -o wide`:  <br>
`kubectl get nodes -0 json`:   <br>
`kubectl get nodes -o yaml`:  <br>
`kubectl get pods`: Lists all pods in the current namespace.  <br>

`kubectl describe nodes`: <br>
`kubectl describe pods`  <br>
`kubectl describe namespace`  <br>
`kubectl describe nodes master1`: <br>
`kubectl get pods -A` <br>


kubectl run -h  <br>
kubectl run pod1 --image nginx  <br>
kubectl run web1 --image nginx --labels run=web1 <br>
kubectl run pod10 --image ubuntu  <br>
kubectl run --image ubuntu pod11 -- "sleep 100"  <br>
kubectl run pod1 --image nginx --namespace ns1  <br>
kubectl run pod1 --image nginx --dry-run=client  <br>
kubectl run pod1 --image nginx --dry-run=client -o yaml  <br>
kubectl create deployment dep1 --replicas 20 --image nginx <br>


kubectl describe pod pod7  <br>

kubectl delete pod pod1  <br>
kubectl delete pods --all  # delete all pod from current namespace; by default default namespace  <br>
kubectl delete pods --all -n kube-system #   <br>
kubectl delete pods --all -n ns1  <br>


kubectl logs pod/pod1  <br>


### Objects In Kubernetes
```bash
kubectl api-resources
```

### Namespace 
In Kubernetes, namespaces provide a mechanism for isolating groups of resources within a single cluster. Names of resources need to be unique within a namespace, but not across namespaces. Namespace-based scoping is applicable only for namespaced objects (e.g. Deployments, Services, etc.) and not for cluster-wide objects (e.g. StorageClass, Nodes, PersistentVolumes, etc.).
```bash
kubectl explain namespace
kubectl get namespaces/ns
```
Create/Delete Namespace
```bash
kubectl create ns prod1
kubectl describe ns prod1
kubectl delete ns prod1
```


### Pod Run using yml file 
```bash
kubectl explain pod
```

#### sigle pod
```yml
---
apiVersion: v1
kind: Pod
metadata:
  name: singlepod
spec:
  containers:
    - name: container1
      image: httpd
```
```bash
kubectl apply -f singlepod.yaml --dry-run=client
kubectl apply -f singlepod.yaml
```
```bash
kubectl exec -it pods/singlepod -- /bin/bash
```

#### Multiple container
```bash
---
apiVersion: v1
kind: Pod
metadata:
  name: multiplecontainer
spec:
 containers:
   - name: doc1
     image: nginx
   - name: doc2
     image: redis
```
```bash
kubectl apply -f multicontainer.yaml --dry-run=client
kubectl apply -f multicontainer.yaml
```
```bash
 kubectl exec -it pods/multiplecontainer11 -c doc1 -- /bin/bash
 kubectl exec -it pods/multiplecontainer11 -c doc2 -- /bin/bash
```



### Taint
Trun off worker node

create pod
```bash
kubectl run pod1 --image nginx  # it will be in pending state
```

check
```bash
kubectl describe nodes master1 | grep -i taint
```
untaint
```bash
kubectl taint node master1 node-role.kubernetes.io/control-plane:NoSchedule-
```
After untaint the pod will run in master node


### Troubleshooting
```bash
kubectl create deployment dep1 --image nginx --replicas 10
kubectl get pods -o wide
kubectl get pods -o wide | grep -i worker1
```
we will see multiple pods will be restarted

To prevent pod restart 
```bash
cd /etc/default
vi kubelet

 KUBELET_EXTRA_ARGS="--cgroup-driver=cgroupfs"

systemctl daemon-reload
systemctl restart kubelet.service
```
pod gulo ke CPU Memory allocation kore systemd & cgroup. We will fix this to cgroup.
```bash
kubectl delete deployments dep1
```


### Edit Pod

Remember, we CANNOT edit specifications of an existing POD other than the below.
- `spec.containers[*].image`
- `spec.initContainers[*].image`
- `spec.activeDeadlineSeconds`
- `spec.tolerations`

#### Edit Name
```bash
kubectl run pod1 --image nginx
kubectl edit pods pod1
  name: pod10  # change name pod1  >pod10
```

An Yaml file named pod10 created in /tmp
<img width="719" height="62" alt="image" src="https://github.com/user-attachments/assets/8cebab60-5e6b-4173-ba84-0c16e174441f" />

We can run this file from /tmp
```bash
kubectl apply -f /tmp/kubectl-edit-1644418017.yaml
kubectl get pods
```

#### Edit Image
```bash
kubectl edit pods pod1
  image: httpd   #change image nginx to httpd 
```

#### Add Container in existing pod; Rule:1
```bash
kubectl edit pods pod1
  # Add this line
  - name: redis
    image: redis
```
<img width="723" height="239" alt="image" src="https://github.com/user-attachments/assets/e5fa4775-c7a5-4de2-9697-7d2fb8a3742c" />

```bash
kubectl delete pod pod1
kubectl apply -f /tmp/kubectl-edit-1602094205.yaml
kubectl describe pods pod1 | grep -i image
```


### Add Container in existing pod; Rule:2
```bash
kubectl set image pods/pod1 pod1=nginx   # here pod1=nginx; pod1 is container name
kubectl set image pods/pod1 redis=httpd
kubectl describe pods pod1
```


### Kubectl apply/create/replace
```yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
    - name: doc1
      image: nginx
```
```bash
kubectl apply -f pod1.yaml --dry-run=client
```

create is for newly resource, if exist it will warning
```bash
kubectl create -f pod1.yaml
```

If same resource exist, it will update the resource
```bash
kubectl apply -f pod1.yaml
```

If i change the name doc1 to doc2
```bash
apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
    - name: doc2
      image: nginx
```
```bash
kubectl replace -f pod1.yaml --force
kubectl describe pod pod1
```


### Execute command in container of a pod or get a shell of container

```bash
kubectl exec pods/<podname> -- <command>
kubectl exec pods/pod1 -- date
kubectl exec pods/pod1 -- hostname

kubectl exec -it pods/<podname> -- <command>
kubectl exec -it pods/pod1 -- /bin/bash
```

If pod have multiple container
```bash
kubectl describe pod pod1
kubectl exec pods/<podname> -c <containername> -- <command>
```

### Copy from host to pod
Create a file name index.html in master ndoe and write `welcome to CKA`

```bash
kubectl get pods -o wide
kubectl cp index.html pod1:/usr/share/nginx/html
curl <pod ip>
kubectl exec pods/pod1 -- ls -l /usr/share/nginx/html/index.html


for multiple container
kubectl cp index.html pod1:/usr/share/nginx/html -c <containername>
kubectl cp index.html pod1:/usr/share/nginx/html -c doc1

```

### Copy from container to node

```bash
kubectl cp pod1:/usr/share/nginx/html ./
```

for multiple container
```bash
kubectl describe pod pod1  # find the container name
kubectl cp pod1:/usr/share/nginx/html -c <containername> ./
kubectl cp pod1:/usr/share/nginx/html -c doc1 ./

```


### Copy directory from host to pod
```bash
kubectl cp dhaka/ pod1:/tmp
kubectl exec pod1 -- ls -l /tmp
kubectl exec pod1 -- ls -l /tmp/dhaka
```

### Logs

```bash
kubectl logs <podname>
kubectl logs/<podname>
kubectl logs pod1

kubectl logs <podname> -c <containername>
kubectl logs pod1 -c doc1

kubectl logs pods/pod1 -c doc1 --since=1h
kubectl logs pods/pod1 -c doc1 --tail=10

kubectl logs pods/pod1 -c doc1 --follow
#curl from worker node
```

### SideCar Container
Sidecar containers are the secondary containers that run along with the main application container within the same Pod.

A sidecar container can collect logs generated by the primary containers and forward them to a centralized logging system.

It can also capture metrics and monitoring data from the primary containers and send them to a monitoring system or dashboard.



### initContainer
specialized containers that run before app containers in a Pod.

Init containers can contain utilities or setup scripts not present in an app image. You can specify init containers in the Pod specification alongside the containers array (which describes app containers).
<img width="794" height="589" alt="image" src="https://github.com/user-attachments/assets/730ab349-7c52-470e-a1af-4d653b812f8c" />

```yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
    - name: container1
      image: nginx
  initContainers:
    - name: container2
      image: ubuntu
```
```bash
kubectl apply -f initContainer.yaml
kubectl describe pod pod1 | more
```


### Pod Restart Policy
- Never (never restart)
- Always (by default)
- OnFailure (If fail, then restart)

#### Never
```bash
ubectl run never --image ubuntu --restart Never
```

#### Always
```bash
kubectl run always --image ubuntu
```

Example: it will restart after 10 sec
```bash
kubectl run ubuntu --image ubuntu -- sleep 10
kubectl get pods -w
```

#### OnFailure
```bash
kubectl run onfailure --image ubuntu --restart OnFailure
```
jodi akta poder moddhe dui ta container thake, and akta container e jodi issue hoi taile sudhu sei container restart hobe. onno container restarrt hobe na

<img width="617" height="122" alt="image" src="https://github.com/user-attachments/assets/670e7843-e058-4ed7-8414-40312c61dd58" />


### Kubernetes service
In Kubernetes, a Service is a method for exposing a network application that is running as one or more Pods in your cluster.

Since pods are ephemeral, a service enables a group of pods, which provide specific functions (web services, image processing, etc.) to be assigned a name and unique IP address (clusterIP). As long as the service is running that IP address, it will not change. Services also define policies for their access. 

#### Types of service
- ClusterIP
- NodePort
- Loadbalancer
- ExternalName

#### ClusterIP
The ClusterIP provides a load-balanced IP address. One or more pods that match a label `selector`   can forward traffic to the IP address. The ClusterIP service must define one or more ports to listen on with target ports to forward TCP/UDP traffic to containers.

ClusterIP Services allow internal connectivity between different applications within the cluster, providing a stable IP address for communication.

- Use case: Internal communication inside the cluster.
- Example: Your backend app talks to the database using an internal address.
<img width="851" height="559" alt="image" src="https://github.com/user-attachments/assets/b11763a9-151d-4bd8-95ed-ff6f8158ddff" />

<img width="945" height="497" alt="image" src="https://github.com/user-attachments/assets/84b9748e-1004-4fc2-aeba-1dd89b530bda" />

```bash
kubectl run webapp --image nginx
kubectl get pods -o wide

vi index.html
  webpod

kubectl cp index.html webapp:/usr/share/nginx/html
curl <podIP>
```

```bash
kubectl get service
```

- nodeport: externaly listen
- port: service koto number port a listen korbe
- targetpor: container er koto number port e jabe

```bash
kubectl create service clusterip svc1 --tcp 80
# service port 80 & target port 80


#Check labels
kubectl get service
kubectl get service svc1 --show-labels
kubectl describe service svc1
kubectl get pod webapp --show-labels
# Here we have to keep same both service labels and pod labels.
# We can change service labels as well pod labels also
# If we edit the service we will change the silector
```

Labels change
```bash
kubectl edit pod webapp  #same as service labels
  app: svc1

kubectl get pod webapp --show-labels

# Or
kubectl label pod webapp app=svc1    # if not work try below
kubectl label pod webapp app=svc1 --overwrite


curl <serviceIP>
```
<img width="484" height="211" alt="image" src="https://github.com/user-attachments/assets/64d73de1-460b-4594-ac0d-257cade34ecc" />


We can run another webapp and change the label. We will see the service is working like loadbalancer
```bash
kubectl run webapp --image nginx

vi index.html
  webpod2

kubectl cp index.html webapp2:/usr/share/nginx/html

kubectl edit pod webapp2  #same as service labels
  app: svc1

kubectl get pod webapp --show-labels

curl <serviceIP>
```
We can see webpod and webpod2

Here we manually set the IP and 81 is service IP, 80 is container IP 
```bash
kubectl delete service svc1
kubectl create service clusterip svc1 --clusterip 10.106.234.80 --tcp 81:80
# service er 81 port e request asle container er 80 port e jabe

curl 10.106.234.80:81
```

It create service first then pod
```bash
kubectl run pod1 --image nginx --port 80 --expose
curl <serviceIP>
```

Edit service
```bash
kubectl edit service svc1
```


#### Another Example of Service

service2:80 >> Container:80    <br>
Service2:81 >> Container:80 

Create new service name svc2
```bash
kubectl create service clusterip svc2 --tcp 80 -o yaml --dry-run=client
kubectl create service clusterip svc2 --tcp 80
kubectl get service
```

Edit Service, We want to listen service port 80 & 81
```bash
# add this line under port
  - name: "port2"
    port: 81
    protocol: TCP
    targetPort: 80
```
<img width="894" height="239" alt="image" src="https://github.com/user-attachments/assets/f6346b9a-23cb-4051-822c-2279ddb0a351" />


Create pod with label
```bash
kubectl run webapp1 --image nginx --labels app=svc2
kubectl run webapp2 --image nginx --labels app=svc2

#check label
kubectl get pod  --show-labels
```

Edit html page
```bash
vi index.html
  webapp1
kubectl cp index.html webapp1:/usr/share/nginx/html


vi index.html
  webapp2
kubectl cp index.html webapp2:/usr/share/nginx/html
```

Curl
```bash
curl <serviceIP>
curl <serviceIP>:81
```
Both pod are listen from 80 & 81


#### NodePort
NodePort builds on top of the ClusterIP Service and provides a way to expose a group of Pods to the outside world. At the API level, the only difference from the ClusterIP is the mandatory service type which has to be set to NodePort; the rest of the values can remain the same.

ClusterIP Services allow internal connectivity between different applications within the cluster, providing a stable IP address for communication.

- Use case: Access the app from outside the cluster (like your browser).
- Explanation: Kubernetes opens a specific port (like 30007) on every worker node, forwarding requests to your app inside the cluster.

<img width="765" height="435" alt="image" src="https://github.com/user-attachments/assets/652863a6-a364-485c-89f8-9a3bc136790f" />

Nodeport range 30000 to 32767

```bash
kubectl create service nodeport ndp1 --tcp 80 -o yaml --dry-run=client    # if node port not define it will take autometically
kubectl create service nodeport ndp1 --tcp 80 --node-port 30100 -o yaml --dry-run=client

kubectl create service nodeport ndp1 --tcp 80 --node-port 30100
kubectl get service ndp1 --show-labels

#Pod create
kubectl run web1 --image nginx --labels app=ndp1

# check from browser
http://192.168.10.100:30100/  # masternode ip
```

Rules: nodeport>>port>>targetport  30200:80:80


#### Loadbalancer
he LoadBalancer service type is built on top of NodePort service types by provisioning and configuring external load balancers from public and private cloud providers. It exposes services that are running in the cluster by forwarding layer 4 traffic to worker nodes. This is a dynamic way of implementing a case that involves external load balancers and NodePort type services.

LoadBalancer service is specially for cloud platforms.

- Use case: Used in cloud environments (AWS, GCP, Azure) to get a public IP.
- Explanation: It automatically creates a cloud load balancer that distributes external traffic across Pods.

<img width="839" height="506" alt="image" src="https://github.com/user-attachments/assets/d6225f8b-7260-4772-8920-df676793cb28" />


#### ExternalName
The ExternalName service in Kubernetes is a specialized type of Service that allows you to map an in-cluster service name to an external hostname or IP address outside of your Kubernetes cluster. This feature enables seamless integration between your Kubernetes applications and external services or resources, providing a bridge between your containerized workloads and the external environment.

The ExternalName service is particularly useful in scenarios where you need to access services hosted outside your Kubernetes cluster, migrate legacy applications to Kubernetes while maintaining dependencies on external systems, or create hybrid deployments where some components run in Kubernetes while others run on external systems. By mapping an internal service name to an external resource, you can transparently access external services from within your Kubernetes applications, simplifying the integration process and enabling a smoother transition to a containerized environment.

- Use case: Connect your app to an external service (like a database hosted elsewhere).
- Explanation: It maps a Kubernetes Service name to an external DNS name (like api.external.com).

 Kichu service kubernetes cluster a cholte pare abar database physical host e cholte pare

<img width="636" height="313" alt="image" src="https://github.com/user-attachments/assets/633b56f2-a1e1-4412-bffa-55bfa2bef39e" />

ExternalName have no load balancer


```bash
kubectl create service externalname ext1 --external-name 192.168.10.152 --tcp 80
```

