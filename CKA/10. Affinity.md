
## Affinity
- nodeAffinity
- podAffinity
You can indicate that a rule is soft or preferred, so that the scheduler still schedules the Pod even if it can't find a matching node.

### nodeAffinity
There are two types of node affinity:
- 1. requiredDuringSchedulingIgnoredDuringExecution:
- 2. preferredDuringSchedulingIgnoredDuringExecution:
 
#### requiredDuringSchedulingIgnoredDuringExecution: 
The scheduler can't schedule the Pod unless the rule is met. Like nodeSelector, but with a more expressive syntax.

`ingIgnoredDu ringExecution`: pod run houar por lebel remove korle pod jeno run kore.

Example: 
- In: ekhane lebel + value diutai dea lagbe
<img width="597" height="254" alt="image" src="https://github.com/user-attachments/assets/fca2bf68-8ffc-4061-8a5f-7adf68cc4113" />

- NotIn: ekhane node er lebel large hole ei node e run korbe na

<img width="675" height="283" alt="image" src="https://github.com/user-attachments/assets/754180b2-d785-4515-a240-52de5bda1516" />
 
- Exists: ekhane label key: size holei run korbe. Value dea jabe na.

<img width="720" height="249" alt="image" src="https://github.com/user-attachments/assets/cdda6764-d38f-4ddc-84e2-f3fc3a3acc32" />

- DoedNotExixr: Jekhane lebel key: size dea thakbe sekhane run korbe na 
<img width="811" height="265" alt="image" src="https://github.com/user-attachments/assets/74eb9152-be14-4a0e-ab1c-8f780235e5d5" />

#### preferredDuringSchedulingIgnoredDuringExecution: 
The scheduler tries to find a node that meets the rule. If a matching node is not available, the scheduler still schedules the Pod.

weight er value 1-100 hote pare.  

Ami prefer korchi nmve te run korbe ,, nvme lebel e run na korle korle onno node e run korbe.

<img width="607" height="255" alt="image" src="https://github.com/user-attachments/assets/17130ec8-b3a8-4aa3-b049-fa664046d0be" />


### podAffinity (Inter-pod affinity/anti-affinity) 
You can constrain a Pod using labels on other Pods running on the node (or other topological domain), instead of just node labels, which allows you to define rules for which Pods can be co-located on a node.

Inter-pod affinity/anti-affinity allows you to constrain Pods against labels on other Pods.

- lebel jodi match kore taile pod same node e run korbe. taile eta affinity
- pod run houar somy jodi kono specifiv lebel kortha bole dea thake tiale and oi lebel ala pod jodi kono node e run kore taile new pod run korar somy ei node run korbe na. label jodi match kore tail pod oi node e run na kore onno node e run korbe. jemon dns1 pod dei node run koreche sei node e jeno dns3 pod run na kore. eta holo pod antiaffinity

zone = Banani
- worker1
- worker2

zone = dhanmondi
- worker3
- worler4

dns1 >> zone banani

dns2 >> zone dhanmondi


kubectl label node kworker1 zone=banani

kubectl label node kworker2 zone=banani

kubectl label node kworker3 zone=dhanmondi

kubectl label node kworker4 zone=dhanmondi

<img width="793" height="562" alt="image" src="https://github.com/user-attachments/assets/da87c056-4812-4268-98cd-3e02483a0d9c" />

akhon 2 ta dns banani ar dhanmondi te run kora ache. ami arekta dns3 banai te run korbo.. kintu bananir je node e dns run kora ache si node run na hoye jenno onno node e run hoi. etar jonno pod affinity
<img width="790" height="367" alt="image" src="https://github.com/user-attachments/assets/03b2da2c-e368-4b76-a6f1-17131ee5e130" />


#### Label set to worker node
```bash
kubectl label nodes worker1 size=small
kubectl label nodes worker2 size=medium
kubectl label nodes worker3 size=large
```

#### Check 
```bash
kubectl get nodes --selector size=small
kubectl get nodes --selector size=medium
kubectl get nodes --selector size=large

kubectl get nodes --show-labels
```

#### Zone set
```bash
kubectl label nodes worker1 zone=banani
kubectl label nodes worker2 zone=banani
kubectl label nodes worker3 zone=dhanmondi
```

#### Zone check
```bash
kubectl get nodes --selector zone=banani
kubectl get nodes --selector zone=dhanmondi
```
### Example:1
```bash
kubectl run pod1 --image nginx -o yaml --dry-run=client > nodename.yaml
```

#### Scheduling disable for worker1
```bash
kubectl cordon worker1
```
<img width="942" height="175" alt="image" src="https://github.com/user-attachments/assets/ac88fdcf-40dc-4e0f-93f7-cdecfd90bca4" />


#### Add nodeName:worker1 
vi nodename.yaml </br>
Add line:</br>
nodeName: worker1
```yaml
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: pod1
  name: pod1
spec:
  nodeName: worker1
  containers:
  - image: nginx
    name: pod1
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```
#### apply & Check 
```bash
kubectl apply -f nodename.yaml
kubectl get pods -o wide
```

But pod1 running on worker1. autometic schedulinh hole worker1 e kono pod run korbe na, but yaml file likhe dile worker1 e run korbe
<img width="1609" height="118" alt="image" src="https://github.com/user-attachments/assets/7a8426eb-1467-493b-a64c-02ab64ad229a" />

#### scheduling on worker1
```bash
kubectl uncordon worker1
```

### Example:2
We want this pod will run on which node label size=large

```bash
kubectl run pod2 --image nginx -o yaml --dry-run=client > nodeselector.yaml
```
vi nodeselector.yaml </br>
Add this line
```bash
nodeSelector:
    size: large
```
vi nodeselector.yaml 
```yaml
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: pod2
  name: pod2
spec:
  nodeSelector:
    size: large
  containers:
  - image: nginx
    name: pod2
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

```bash
kubectl apply -f nodeselector.yaml
kubectl get pods -o wide
```
earliar we set size=large in worker3 
<img width="1450" height="109" alt="image" src="https://github.com/user-attachments/assets/ad04032a-a4ce-41db-92e1-36531ed600e8" />

#### Example:3 
If scheduling is disable for worker node3, then pod will be in pending state

```bash
kubectl cordon worker3
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: pod2
  name: pod2
spec:
  nodeSelector:
    size: large
  containers:
  - image: nginx
    name: pod2
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

```bash
kubectl apply -f nodeselector.yaml
kubectl get pods -o wide
```
<img width="872" height="38" alt="image" src="https://github.com/user-attachments/assets/649d4df9-1b8f-4d48-9310-a2ce4dbe540b" />

#### Example:4
nodeAffinityRequired

```bash
kubectl run web1 --image nginx -o yaml --dry-run=client > nar.yaml
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: web1
  name: web1
spec:
  affinity:
   nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: In
            values:
              - small
  containers:
  - image: nginx
    name: web1
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

```bash
kubectl apply -f nar.yaml
kubectl get pods -o wide
```
<img width="1474" height="132" alt="image" src="https://github.com/user-attachments/assets/6abf4d98-d022-4f05-a34f-fec8695fdb0b" />


#### If worker node 1 is unavailable it will run worker node2 becuase worker node2 is medium
```bash
kubectl cordon worker1
kubectl delete pod web1
```

vi nar.yaml
```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: web1
  name: web1
spec:
  affinity:
   nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: In
            values:
              - small
              - medium

  containers:
  - image: nginx
    name: web1
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

#### apply
```bash
kubectl apply -f nar.yaml
kubectl get pods -o wide
```
<img width="1468" height="161" alt="image" src="https://github.com/user-attachments/assets/9d63b3e0-ec8a-4f92-97e2-e59281f57943" />

```bash
kubectl uncordon worker1
```

### Example:5 
NotIn: Here from yaml code we can see that `NotIn` values are small & medium so this pod will run in `large`

```bash
kubectl delete pod web1
```
vi nar.yaml
```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: web1
  name: web1
spec:
  affinity:
   nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: NotIn
            values:
              - small
              - medium

  containers:
  - image: nginx
    name: web1
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

```bash
kubectl apply -f nar.yaml
#or
kubectl replace -f nar.yaml --force  #if web pod not deleted

kubectl get pods -o wide
```
we can see the web pod run on worker3 which is labeled size=large
<img width="1494" height="166" alt="image" src="https://github.com/user-attachments/assets/59c5e883-3d84-4378-b3b5-25f428ca1d4d" />

### Example:6 
Exists: Exixts thakle value dea lagbe na 

vi nar.yaml
```yaml
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: web1
  name: web1
spec:
  affinity:
   nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: Exists
  containers:
  - image: nginx
    name: web1
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

#### apply
```bash
kubectl apply -f nar.yaml --force
```
The pod will run any node.


#### Example:7 
preferred: 

vi pre.yaml </br>
Here we can use multiple values
```yaml
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: web1
  name: web1
spec:
  affinity:
   nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
         matchExpressions:
          - key: size
            operator: In
            values:
             - large
  containers:
  - image: nginx
    name: web1
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

#### Apply and check 
```bash
kubectl apply -f pre.yaml
kubectl get pods -o wide
```
It will run on worker node3
<img width="1375" height="93" alt="image" src="https://github.com/user-attachments/assets/1e689fad-9c6e-4aab-b1a6-3c7c797d66b5" />

If worker node3 unavailable or if no tag name `large`

```bash
kubectl cordon worker3 # unchedule
# or
kubectl label nodes worker3 size-  # unlabel worker node3
kubectl delete pod --all
```

#### check
```bash
kubectl get nodes --selector size=large
```

#### apply & check 
```bash
kubectl apply -f pre.yaml
kubectl get pods -o wide
```

#### Ulabel nodes
```bash
kubectl label nodes worker1 size-
```
As no node has `large` label the pod running on anywhere
<img width="1428" height="88" alt="image" src="https://github.com/user-attachments/assets/9a4c9983-f1ab-4de3-b06c-eb9be11a9d7f" />



 kubectl delete pod -n kube-flannel -l app=flannel --field-selector spec.nodeName=worker3



 ### podAffinity

- podAffinity
- podAntiAffinity
- taints and tolerations
- static pod
- daemonset

<img width="459" height="278" alt="image" src="https://github.com/user-attachments/assets/86ecd910-75ad-4284-ae2c-68eb682f292c" />



#### `podAffinity`: Same label Pod will run in the same node or same zone – Depend on topologyKey..

#### `podAntiAffinity`: Same label Pod will run in the different node or different zone – Depend on topologyKey.

#### Taints and Tolerations :
- Node affinity is a property of Pods that attracts them to a set of nodes.
- Taints are the opposite –– they allow a node to repel a set of pods.
- Tolerations are applied to pods.
- Tolerations allow the scheduler to schedule pods with matching taints.
- Tolerations allow scheduling but don't guarantee scheduling: the scheduler also evaluates other parameters as part of its function.
- Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints.

#### Taints 
- Taints set to node.
- Taints has 3 parameters: key, value and effect
- There are 3 effects : NoSchedule – PreferNoSchedule – NoExecute

```bash
kubectl run dbpod --image nginx --labels pod=db
```

```bash
kubectl run web --image nginx -o yaml  --dry-run=client > web.yaml
```

Search `podAffinity` in https://kubernetes.io/ </br>

Copy this line
<img width="1423" height="734" alt="image" src="https://github.com/user-attachments/assets/71447912-c7d9-4cb8-9038-5e6ee88fde36" />

For Topology key
```bash
kubectl get nodes --show-labels
kubectl get nodes worker1 --show-lables
```

```yaml
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: web
  name: web
spec:
  spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: pod
            values:
            - db1
        topologyKey: kubernetes.io/hostname
  containers:
  - image: nginx
    name: web1
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

```bash
kubectl apply -f web.yaml --dry-run=client
kubectl apply -f web.yaml
kubectl describe pods
```
We will see the pod will not run 
Change values `db1` to `db`
```yaml
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: web
  name: web
spec:
  spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: pod
            values:
            - db
        topologyKey: kubernetes.io/hostname
  containers:
  - image: nginx
    name: web1
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```
```bash
kubectl replace -f web.yaml --force
```
